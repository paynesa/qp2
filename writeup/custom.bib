@inproceedings{ludic,
  title={Open corpus of Veps and Karelian languages (VepKar): preliminary data collection and dictionaries},
  author={Zaytseva, Nina and Krizhanovsky, Andrew and Krizhanovsky, Natalia and Pellinen, Natalia and Rodionova, Aleksndra},
  booktitle={Corpus Linguistics},
  volume={2017},
  pages={172--177},
  year={2017}
}

@inproceedings{kodner2023re,
  title={Re-Evaluating the Evaluation of Neural Morphological Inflection Models},
  author={Kodner, Jordan and Khalifa, Salam and Payne, Sarah RB and Liu, Zoey},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={45},
  number={45},
  year={2023}
}

@article{lachmy2022,
    author = {Lachmy, Royi and Pyatkin, Valentina and Manevich, Avshalom and Tsarfaty, Reut},
    title = "{Draw Me a Flower: Processing and Grounding Abstraction in Natural Language}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {1341-1356},
    year = {2022},
    month = {11},
    abstract = "{Abstraction is a core tenet of human cognition and communication. When composing natural language instructions, humans naturally evoke abstraction to convey complex procedures in an efficient and concise way. Yet, interpreting and grounding abstraction expressed in NL has not yet been systematically studied in NLP, with no accepted benchmarks specifically eliciting abstraction in NL. In this work, we set the foundation for a systematic study of processing and grounding abstraction in NLP. First, we deliver a novel abstraction elicitation method and present Hexagons, a 2D instruction-following game. Using Hexagons we collected over 4k naturally occurring visually-grounded instructions rich with diverse types of abstractions. From these data, we derive an instruction-to-execution task and assess different types of neural models. Our results show that contemporary models and modeling practices are substantially inferior to human performance, and that model performance is inversely correlated with the level of abstraction, showing less satisfying performance on higher levels of abstraction. These findings are consistent across models and setups, confirming that abstraction is a challenging phenomenon deserving further attention and study in NLP/AI research.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00522},
    url = {https://doi.org/10.1162/tacl\_a\_00522},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00522/2061241/tacl\_a\_00522.pdf},
}

@inproceedings{arora2021,
  title={Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding},
  author={Siddhant Arora and Alissa Ostapenko and Vijay Viswanathan and Siddharth Dalmia and Florian Metze and Shinji Watanabe and Alan W. Black},
  booktitle={Interspeech},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:235669959}
}


@InProceedings{sparsemax,
  title = 	 {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
  author = 	 {Martins, Andre and Astudillo, Ramon},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1614--1623},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/martins16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/martins16.html},
  abstract = 	 {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.}
}
