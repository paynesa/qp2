---
title: "Statistics for Replicating Goldman et al. (2022)"
output: html_notebook
---

This notebook contains code to execute plotting and exploratory statistics for replicating Goldman et al. (2022) on a subset of 3 language families: Uralic, Romance, and Niger-Congo. We begin by investigating predictors of accuracy drop between SIGMORPHON (2020) and Goldman et al. (2022), then investigate the relationship between raw accuracy on the Goldman et al. test data and various predictors.

# Setup 

#### Load Libaries 

```{r}
library(ggplot2)
library(tidyverse)
library(car)
library(mgcv)
require(splines)
```

#### Read in & Process the Data

```{r}
# Read in the data 
df<-read.csv("replication/replication_complete.csv")

# Make the family column pretty 
df$Family = str_to_title(str_replace(df$Family, "_", " "))

# We're going to be making a lot of log-log plots, so lets make our lives easier:
df$log_test_acc_drop = -1*log((-1*df$test_acc_drop)+1)
df$log_train_lemma_diff = -1*log(-1*df$train_lemma_diff_raw)
```

#### Define Helper Functions 

Here, we define helper functions to run our correlational statistics, evaluate our fitted models, and compare models fitted to a single predictor to models fitted to multiple predictors in order to evaluate which better explains the data.

```{r}
# We'll use this helper function to run our stats
correlations <- function(a, b){
  for (m in list("pearson", "spearman", "kendall")){
    # Supressing warnings bc we'll get them whenever there are ties 
    suppressWarnings(res <- cor.test(a, b, method = m))
    formatted <- sprintf("%s: %f (p = %e)", res$method, res$estimate, res$p.value)
    print(formatted)
  }
}

# We'll use this helper function to get information about our model
eval_model <- function(model, df){
  rsquared = summary(model)$r.squared
  AIC = AIC(model)
  results <- sprintf("R^2: %f, AIC: %f", rsquared, AIC)
  print(results)
  layout(matrix(c(1,2,3,4),2,2)) 
  plot(model)
  return(predict(model, df, se = TRUE))
}

# We'll use this helper function to compare two models 
compare_models <- function(model_both, model_single){
  layout(matrix(c(1,2,3,4),2,2)) 
  plot(model_both)
  AIC = AIC(model_both)
  anovap = anova(model_both, model_single)$`Pr(>F)`[-1]
  results <- sprintf("AIC: %f, ANOVA p: %f", AIC, anovap)
  print(results)
  vif(model_both)
}
```

# Investigating Accuracy Drop

We begin by investigating relationships between test accuracy drop (between SIGMORPHON 2020 & Goldman et al 2022) and various predictors. Exploratory analysis tells us that the following predictors have strong relationships with accuracy drop:

-   **Training size:** the number of training triples in the Goldman et al. data

-   **Training lemmas:** the number of unique lemmas occurring in the Goldman et al. data

-   **Lemma drop:** the difference in the number of lemmas between the SIGMORPHON data and the Goldman et al. data.

We begin by investigating unscaled scatter plots before fitting linear models and running statistics on the log-log scaled plots, and finally fitting non-linear models to these predictors.

## Unscaled Scatter Plots

#### Training Size

```{r}
df %>% 
  ggplot(aes(Goldman_train_size, test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size") + 
  ylab("Test accuracy drop") + 
  ggtitle("Replication of Goldman et al.") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
        )
```

#### Training Lemmas

```{r}
df %>% 
  ggplot(aes(Goldman_train_lemmas, test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training lemmas") + 
  ylab("Test accuracy drop") + 
  ggtitle("Test accuracy drop vs. training lemmas") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )
```

#### Training Lemma Drop

```{r}
df %>% 
  ggplot(aes(train_lemma_diff_raw, test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Training lemma difference between SIGMORPHON & Goldman et al.") + 
  ylab("Test accuracy drop") + 
  ggtitle("Test accuracy drop vs. training lemma drop") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )
```

## Linear Models and Correlations

From the above, we can see that the relationship between the various predictors and test accuracy drop isn't linear. However, when both axes are log-scaled, the relationship is near linear. As such, we fit linear models and run correlation statistics on the log-log scaled version.

#### Training size vs. test accuracy drop

```{r}
train_size_lm = lm(log_test_acc_drop ~ log(Goldman_train_size), data = df)
pred <- eval_model(train_size_lm, df)

df %>% 
  ggplot(aes(log(Goldman_train_size), log_test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size, log scale") + 
  ylab("Test accuracy drop, log scale") + 
  ggtitle("Test accuracy drop vs. training size") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  +
  geom_ribbon(aes(x = log(Goldman_train_size), 
                  ymin = pred$fit - 2 * pred$se.fit, 
                  ymax = pred$fit + 2 * pred$se.fit), 
              fill = "grey", 
              alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_size), y = pred$fit), color = "black")
ggsave("../writeup/figs/lm_drop_size.png", dpi=500)

correlations(log(df$Goldman_train_size), df$log_test_acc_drop)
```

#### Training lemmas vs. test accuracy drop

```{r}
train_lemma_lm = lm(log_test_acc_drop ~ log(Goldman_train_lemmas), data = df)
pred <- eval_model(train_lemma_lm, df)

df %>% 
  ggplot(aes(log(Goldman_train_lemmas), log_test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. number of training lemmas, log scale") + 
  ylab("Test accuracy drop, log scale") + 
  ggtitle("Test accuracy drop vs. number of training lemmas") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log(Goldman_train_lemmas), 
                 ymin = pred$fit - 2 * pred$se.fit, 
                 ymax = pred$fit + 2 * pred$se.fit), 
             fill = "grey", 
             alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_lemmas), y = pred$fit), color = "black")
ggsave("../writeup/figs/lm_drop_lemmas.png", dpi=500)

correlations(log(df$Goldman_train_lemmas), df$log_test_acc_drop)
```

#### Training lemma drop vs. test accuracy drop

```{r}
lemma_drop_lm = lm(log_test_acc_drop ~ log_train_lemma_diff, data = df)
pred <- eval_model(lemma_drop_lm, df)

df %>% 
  ggplot(aes(log_train_lemma_diff, log_test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Training lemma drop, log scale") + 
  ylab("Test accuracy drop, log scale") + 
  ggtitle("Test accuracy drop vs. training lemma drop") +
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log_train_lemma_diff, 
                 ymin = pred$fit - 2 * pred$se.fit, 
                 ymax = pred$fit + 2 * pred$se.fit), 
             fill = "grey", 
             alpha = .4) + 
  geom_line(aes(x = log_train_lemma_diff, y = pred$fit), color = "black")

ggsave("../writeup/figs/lm_drop_drop.png", dpi=500)
correlations(df$log_train_lemma_diff, df$log_test_acc_drop)
```

## Co-Linearity

Intuitively, it makes sense that several of the possible predictors above would be co-linear: larger training data will generally contain more lemmas, for example. We investigate these co-linearities below.

#### Training size vs. training lemmas

```{r}
df %>% 
  ggplot(aes(log(Goldman_train_size), log(Goldman_train_lemmas))) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size, log scale") + 
  ylab("Goldman et al. number of training lemmas, log scale") + 
  ggtitle("Training size vs. training lemmas, log scale") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  stat_smooth(method="lm", color="black", size=0.5, alpha = 0.5)

correlations(log(df$Goldman_train_size), log(df$Goldman_train_lemmas))
```

#### Training size vs. difference in training lemmas

```{r}
df %>%
  ggplot(aes(log(Goldman_train_size), log_train_lemma_diff)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size, log scale") + 
  ylab("Training lemma difference, log scale") + 
  ggtitle("Training size vs. training lemma drop") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  stat_smooth(method="lm", color="black", size=0.5, alpha = 0.5)

correlations(log(df$Goldman_train_size), df$log_train_lemma_diff)
```

#### Training lemmas vs. difference in training lemmas

```{r}
df %>%
  ggplot(aes(log(Goldman_train_lemmas), log_train_lemma_diff)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training lemmas, log scale") + 
  ylab("Training lemma difference, log scale") + 
  ggtitle("Training lemmas vs. training lemma drop") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  stat_smooth(method="lm", color="black", size=0.5, alpha = 0.5)

correlations(log(df$Goldman_train_lemmas), df$log_train_lemma_diff)
```

## Accuracy Drop Pirate Stats, Linear Version üè¥‚Äç‚ò†Ô∏è

We now have three possible predictor variables which we know are co-linear with one another, so we want to disentangle these relationships to determine which variable(s) best predict drops in model performance.

Above, we saw that of the single-predictor linear models, test accuracy drop was best predicted by training lemmas. Using this as a starting point, we add additional predictors to the linear models to determine if any additional predictor leads to significantly better model performance, defined as follows:

-   Does the AIC drop by at least 2 units (significantly better)?¬†

-   Is the ANOVA p-value between the two models significant? (p \< 0.05)

-   Is the VIF (a measure of co-linearity) low ‚Äì below 3-4?

#### Lemmas + Train Size

We compare a model conditioned on the number of lemmas in train *and* the number of triples in train to one just conditioned on the number of lemmas.

```{r}
lemmas_plus_train_lm = lm(log_test_acc_drop ~ log(Goldman_train_size) + log(Goldman_train_lemmas), data = df)
summary(lemmas_plus_train_lm)$r.squared
compare_models(lemmas_plus_train_lm, train_lemma_lm)
```

#### Lemmas + Lemma Drop

We compare a model conditioned on the number of lemmas in train *and* the lemma drop between SIGMORPHON and Goldman et al in train to one just conditioned on the number of lemmas.

```{r}
lemmas_drop_lm = lm(log_test_acc_drop ~ log(Goldman_train_lemmas) + log_train_lemma_diff, data = df)
summary(lemmas_drop_lm)$r.squared
compare_models(lemmas_drop_lm, train_lemma_lm)
```

## Accuracy Drop Pirate Stats, Non-Linear Version üè¥‚Äç‚ò†Ô∏è

Though the relationship is near-linear when we log-scale both axes, we can see from our residual plots above that there is some non-linearity remaining. As such, we train more general models using the natural cubic splines with `df = 3`.

#### Training size

```{r}
ns_train_size_lm = lm(log_test_acc_drop ~ ns(log(Goldman_train_size), df=3), data = df)
pred <- eval_model(ns_train_size_lm, df)

df %>% 
  ggplot(aes(log(Goldman_train_size), log_test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size, log scale") + 
  ylab("Test accuracy drop, log scale") + 
  ggtitle("Test accuracy drop vs. training size") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  +
  geom_ribbon(aes(x = log(Goldman_train_size), 
                  ymin = pred$fit - 2 * pred$se.fit, 
                  ymax = pred$fit + 2 * pred$se.fit), 
              fill = "grey", 
              alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_size), y = pred$fit), color = "black")

ggsave("../writeup/figs/ns_drop_size.png", dpi=500)
```

#### Train lemmas

```{r}
ns_train_lemma_lm = lm(log_test_acc_drop ~ ns(log(Goldman_train_lemmas), df = 3), data = df)
pred <- eval_model(ns_train_lemma_lm, df)

df %>% 
  ggplot(aes(log(Goldman_train_lemmas), log_test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. number of training lemmas, log scale") + 
  ylab("Test accuracy drop, log scale") + 
  ggtitle("Test accuracy drop vs. number of training lemmas") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log(Goldman_train_lemmas), 
                 ymin = pred$fit - 2 * pred$se.fit, 
                 ymax = pred$fit + 2 * pred$se.fit), 
             fill = "grey", 
             alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_lemmas), y = pred$fit), color = "black")

ggsave("../writeup/figs/ns_drop_lemmas.png", dpi=500)
```

#### Training Lemma Drop

```{r}
ns_lemma_drop_lm = lm(log_test_acc_drop ~ ns(log_train_lemma_diff, df = 3), data = df)
pred <- eval_model(ns_lemma_drop_lm, df)

df %>% 
  ggplot(aes(log_train_lemma_diff, log_test_acc_drop)) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Training lemma drop, log scale") + 
  ylab("Test accuracy drop, log scale") + 
  ggtitle("Test accuracy drop vs. training lemma drop") +
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log_train_lemma_diff, 
                 ymin = pred$fit - 2 * pred$se.fit, 
                 ymax = pred$fit + 2 * pred$se.fit), 
             fill = "grey", 
             alpha = .4) + 
  geom_line(aes(x = log_train_lemma_diff, y = pred$fit), color = "black")

ggsave("../writeup/figs/ns_drop_drop.png", dpi=500)
```

Having trained the preliminary single-predictor models, we can once again train the more complex multi-predictor models and compare them in terms of AIC, ANOVA p-value, and VIF as above:

#### Lemmas + Train Size

```{r}
ns_lemmas_plus_train_lm = lm(log_test_acc_drop ~ ns(log(Goldman_train_size), df = 3) + ns(log(Goldman_train_lemmas), df = 3), data = df)
summary(ns_lemmas_plus_train_lm)$r.squared
compare_models(ns_lemmas_plus_train_lm, ns_train_lemma_lm)
```

#### Lemmas + Lemma Drop

```{r}
ns_lemmas_drop_lm = lm(log_test_acc_drop ~ ns(log(Goldman_train_lemmas), df = 3) + ns(log_train_lemma_diff, df = 3), data = df)
summary(ns_lemmas_drop_lm)$r.squared
compare_models(ns_lemmas_drop_lm, ns_train_lemma_lm)
```

# Investigating Raw Accuracy

Having investigated the relationships with accuracy drop, we now wish to understand the predictors of raw accuracy. To begin, we visualize the relationship with raw accuracy for both SIGMORPHON and Goldman et al.

## Basic Visualization

First, format the data for easier plotting:

```{r}
Gold <- df %>% 
  select(train=Goldman_train_size, 
         lemmas = Goldman_train_lemmas,
         test = Goldman_test_acc, 
         Family = Family
         ) 
Gold$Type = "Goldman"

Sigm <- df %>% 
  select(train = SIGMORPHON_train_size,
         lemmas = SIGMORPHON_train_lemmas,
         test = SIGMORPHON_test_acc,
         Family = Family
         )
Sigm$Type = "SIGMORPHON"

new_df = rbind(Gold, Sigm)
```

#### Training Size

```{r}
new_df %>% 
  ggplot(aes(log(train), log(test + 1), color = Type)) +
  geom_point(aes(shape = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  stat_smooth(aes(color=Type), method="lm", size=0.5, alpha = 0.5)+ 
  theme_bw() + 
  xlab("Training size, log scale") + 
  ylab("Test accuracy, log scale") + 
  ggtitle("Test accuracy vs. training size") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  ) 
print("-------GOLDMAN ------")
correlations(log(df$Goldman_train_size), log(df$Goldman_test_acc + 1))
print("-------SIGMORPHON -------")
correlations(log(df$SIGMORPHON_train_size), log(df$SIGMORPHON_test_acc + 1))
```

#### Training Lemmas

```{r}
new_df %>% 
  ggplot(aes(log(lemmas), log(test + 1), color = Type)) +
  geom_point(aes(shape = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  stat_smooth(aes(color=Type), method="lm", size=0.5, alpha = 0.5) + 
  theme_bw() + 
  xlab("Training lemmas, log scale") + 
  ylab("Test accuracy, log scale") + 
  ggtitle("Test accuracy vs. training lemmas") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  ) 
print("-------GOLDMAN ------")
correlations(log(df$Goldman_train_lemmas), log(df$Goldman_test_acc + 1))
print("-------SIGMORPHON -------")
correlations(log(df$SIGMORPHON_train_lemmas), log(df$SIGMORPHON_test_acc + 1))
```

#### Co-Linearities 

```{r}
print("-------GOLDMAN ------")
correlations(log(df$Goldman_train_size), log(df$Goldman_train_lemmas))
print("-------SIGMORPHON -------")
correlations(log(df$SIGMORPHON_train_size), log(df$SIGMORPHON_train_lemmas))
```

## Raw Accuracy Pirate Stats, Linear Version üè¥‚Äç‚ò†Ô∏è

As we did for accuracy drop, we now fit linear models to raw accuracy on the Goldman et al. data only (since there seems to be little effect for the SIGMORPHON data). We then compare the models fitted to just training size or training lemmas to the model fitted to both in the same way as above.

#### Training Size

```{r}
train_size_lm = lm(log(Goldman_test_acc + 1) ~ log(Goldman_train_size), data = df)
pred <- eval_model(train_size_lm, df)

df %>% 
  ggplot(aes(log(Goldman_train_size), log(Goldman_test_acc + 1))) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size, log scale") + 
  ylab("Goldman et al. test accuracy, log scale") + 
  ggtitle("Goldman et al. test accuracy vs. training size") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log(Goldman_train_size), 
                  ymin = pred$fit - 2 * pred$se.fit, 
                  ymax = pred$fit + 2 * pred$se.fit), 
              fill = "grey", 
              alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_size), y = pred$fit), color = "black") 
ggsave("../writeup/figs/lm_raw_size.png", dpi=500)
```

#### Number of Training Lemmas

```{r}
train_lemma_lm = lm(log(Goldman_test_acc + 1) ~ log(Goldman_train_lemmas), data = df)
pred <- eval_model(train_lemma_lm)

df %>% 
  ggplot(aes(log(Goldman_train_lemmas), log(Goldman_test_acc + 1))) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. number of training lemmas, log scale") + 
  ylab("Goldman et al. test accuracy, log scale") + 
  ggtitle("Goldman et al. test accuracy vs. training lemmas") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log(Goldman_train_lemmas), 
                  ymin = pred$fit - 2 * pred$se.fit, 
                  ymax = pred$fit + 2 * pred$se.fit), 
              fill = "grey", 
              alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_lemmas), y = predict(train_lemma_lm)), color = "black")
ggsave("../writeup/figs/lm_raw_lemmas.png", dpi=500)
```

#### Training Size & Number of Training Lemmas

```{r}
lemmas_plus_train_lm <- lm(log(Goldman_test_acc + 1) ~ log(Goldman_train_size) + log(Goldman_train_lemmas), data = df)
summary(lemmas_plus_train_lm)$r.squared
compare_models(lemmas_plus_train_lm, train_lemma_lm)
compare_models(lemmas_plus_train_lm, train_size_lm)
```

## Raw Accuracy Pirate Stats, Non-Linear Version üè¥‚Äç‚ò†Ô∏è

It's very clear from the above plots ‚Äì more so than for test accuracy drop ‚Äì that the relationships aren't linear even on the log-log scale. As such, we once again make use of a natural cubic spline with `df = 3` to extend these models to be non-linear and fit two single predictor models and one two-predictor model as above.

#### Training Size

```{r}
ns_train_size_lm = lm(log(Goldman_test_acc + 1) ~ ns(log(Goldman_train_size), df = 3), data = df)
pred <- eval_model(ns_train_size_lm, df)
df %>% 
  ggplot(aes(log(Goldman_train_size), log(Goldman_test_acc + 1))) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. training size, log scale") + 
  ylab("Goldman et al. test accuracy, log scale") + 
  ggtitle("Goldman et al. test accuracy vs. training size") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log(Goldman_train_size), 
                  ymin = pred$fit - 2 * pred$se.fit, 
                  ymax = pred$fit + 2 * pred$se.fit), 
              fill = "grey", 
              alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_size), y = pred$fit), color = "black")
ggsave("../writeup/figs/ns_raw_size.png", dpi=500)
```

#### Training Lemmas 

```{r}
ns_train_lemma = lm(log(Goldman_test_acc + 1) ~ ns(log(Goldman_train_lemmas), df = 3), data = df)
pred <-  eval_model(ns_train_lemma, df)
df %>% 
  ggplot(aes(log(Goldman_train_lemmas), log(Goldman_test_acc + 1))) + 
  geom_point(aes(colour = Family), size = 5, alpha = 0.5) + 
  scale_color_manual(values=c("turquoise", "purple", "gold")) + 
  theme_bw() + 
  xlab("Goldman et al. number of training lemmas, log scale") + 
  ylab("Goldman et al. test accuracy, log scale") + 
  ggtitle("Goldman et al. test accuracy vs. training lemmas") + 
  theme(plot.title = element_text(hjust=0.5, size=18), 
        axis.title.y = element_text(size=14),
        axis.title.x = element_text(size=14),
  )  + 
  geom_ribbon(aes(x = log(Goldman_train_lemmas), 
                  ymin = pred$fit - 2 * pred$se.fit, 
                  ymax = pred$fit + 2 * pred$se.fit), 
              fill = "grey", 
              alpha = .4) + 
  geom_line(aes(x = log(Goldman_train_lemmas), y = pred$fit), color = "black")

ggsave("../writeup/figs/ns_raw_lemmas.png", dpi=500)
```

#### Training Size + Training Lemmas

```{r}
ns_lemmas_plus_train_lm <- lm(log(Goldman_test_acc + 1) ~ ns(log(Goldman_train_size), df = 3) + ns(log(Goldman_train_lemmas), df = 3), data = df)
summary(ns_lemmas_plus_train_lm)$r.squared
compare_models(ns_lemmas_plus_train_lm, ns_train_lemma)
compare_models(ns_lemmas_plus_train_lm, ns_train_size_lm)
```
